# -*- coding: utf-8 -*-
# Copyright Â© 2019 Apple Inc. All rights reserved.
#
# Use of this source code is governed by a BSD-3-clause license that can
# be found in the LICENSE.txt file or at https://opensource.org/licenses/BSD-3-Clause
from __future__ import print_function as _
from __future__ import division as _
from __future__ import absolute_import as _


import tensorflow as _tf 
from .._tf_model import TensorFlowModel
import turicreate.toolkits._tf_utils as _utils

import numpy as _np

# Constant parameters for the neural network
CONV_H = 64
LSTM_H = 200
DENSE_H = 128
        
class ActivityTensorFlowModel(TensorFlowModel):
    
    def __init__(self, net_params, batch_size, num_features, num_classes, prediction_window, seq_len):

        for key in net_params.keys():
            net_params[key] = _utils.convert_shared_float_array_to_numpy(net_params[key])

         # Suppresses verbosity to only errors
        _tf.compat.v1.logging.set_verbosity(_tf.compat.v1.logging.ERROR)
        
        _tf.reset_default_graph()

        self.num_classes = num_classes
        self.batch_size = batch_size
        self.seq_len = seq_len
        
        # Vars
        self.data = _tf.compat.v1.placeholder(_tf.float32, [None, prediction_window*seq_len, num_features])
        self.weight = _tf.compat.v1.placeholder(_tf.float32, [None, seq_len, 1])
        self.target = _tf.compat.v1.placeholder(_tf.int32, [None, seq_len, 1])
        self.is_training = _tf.compat.v1.placeholder(_tf.bool)
        
        # Reshaping weights 
        reshaped_weight = _tf.reshape(self.weight, [self.batch_size, seq_len])

        # One hot encoding target
        reshaped_target = _tf.reshape(self.target, [self.batch_size, seq_len])
        one_hot_target = _tf.one_hot(reshaped_target, depth=self.num_classes, axis=-1)
        
        # Weights 
        self.weights = {
        'conv_weight' : _tf.Variable(_tf.zeros([prediction_window, num_features, CONV_H]), name='conv_weight'),
        'dense0_weight': _tf.Variable(_tf.zeros([LSTM_H, DENSE_H]), name='dense0_weight'),
        'dense1_weight'  : _tf.Variable(_tf.zeros([DENSE_H, self.num_classes]), name='dense1_weight')
        }

        # Biases
        self.biases = {
        'conv_bias' : _tf.Variable(_tf.zeros([CONV_H]), name='conv_bias'),
        'dense0_bias': _tf.Variable(_tf.zeros([DENSE_H]), name='dense0_bias'),
        'dense1_bias'  : _tf.Variable(_tf.zeros([num_classes]), name='dense1_bias')
        }

        # Convolution 
        conv = _tf.nn.conv1d(self.data, self.weights['conv_weight'], stride=prediction_window, padding='SAME')
        conv = _tf.nn.bias_add(conv, self.biases['conv_bias'])
        conv = _tf.nn.relu(conv)

        dropout = _tf.layers.dropout(conv, rate=0.2, training=self.is_training)

        # Long Stem Term Memory
        lstm = self.load_lstm_weights_params(net_params)
        cells = _tf.nn.rnn_cell.LSTMCell(num_units=LSTM_H, reuse=_tf.AUTO_REUSE, forget_bias=0.0, 
            initializer=_tf.initializers.constant(lstm, verify_shape=True))
        init_state = cells.zero_state(batch_size, _tf.float32)
        rnn_outputs, final_state = _tf.nn.dynamic_rnn(cells, dropout, initial_state=init_state)

        # Dense
        dense = _tf.reshape(rnn_outputs, (-1, LSTM_H))
        dense = _tf.add(_tf.matmul(dense, self.weights['dense0_weight']), self.biases['dense0_bias'])
        dense = _tf.layers.batch_normalization(inputs=dense, 
            beta_initializer=_tf.initializers.constant(net_params['bn_beta'], verify_shape=True), 
            gamma_initializer=_tf.initializers.constant(net_params['bn_gamma'], verify_shape=True),
            moving_mean_initializer=_tf.initializers.constant(net_params['bn_running_mean'], verify_shape=True), 
            moving_variance_initializer=_tf.initializers.constant(net_params['bn_running_var'], verify_shape=True), training=self.is_training )
        dense = _tf.nn.relu(dense)
        dense = _tf.layers.dropout(dense, rate=0.5, training=self.is_training)
        
        # Output
        out = _tf.add(_tf.matmul(dense, self.weights['dense1_weight']), self.biases['dense1_bias'])
        out = _tf.reshape(out, (-1, self.seq_len, self.num_classes))
        self.probs = _tf.nn.softmax(out)
        
        # Weights 
        seq_sum_weights = _tf.reduce_sum(reshaped_weight, axis=1)
        binary_seq_sum_weights = _tf.reduce_sum(_tf.cast(seq_sum_weights > 0, dtype=_tf.float32))

        # Loss
        loss = _tf.losses.softmax_cross_entropy(logits=out, onehot_labels=one_hot_target, weights=reshaped_weight, reduction=_tf.losses.Reduction.NONE)
        self.loss_per_seq = _tf.reduce_sum(loss, axis=1) / (seq_sum_weights + 1e-5)
        self.loss_op = _tf.reduce_sum(self.loss_per_seq) / (binary_seq_sum_weights + 1e-5)

        # Optimizer 
        update_ops = _tf.get_collection(_tf.GraphKeys.UPDATE_OPS)
        self.set_learning_rate(1e-3)
        train_op = self.optimizer.minimize(self.loss_op)
        self.train_op = _tf.group([train_op, update_ops])

        # Session 
        self.sess = _tf.compat.v1.Session()

        # Initialize all variables
        self.sess.run(_tf.compat.v1.global_variables_initializer())
        self.sess.run(_tf.compat.v1.local_variables_initializer())
        
        self.load_weights(net_params)

    def load_lstm_weights_params(self, net_params):
        """
        Function to load lstm weights from the C++ implementation into TensorFlow

        Parameters
        ----------
        net_params: Dictionary
            Dict with weights from the C++ implementation and  its names

        Returns
        -------

        lstm: lstm weights in Tensorflow Format
        """
        i2h_i = net_params['lstm_i2h_i_weight']
        i2h_f = net_params['lstm_i2h_f_weight']
        i2h_c = net_params['lstm_i2h_c_weight']
        i2h_o = net_params['lstm_i2h_o_weight']
        h2h_i = net_params['lstm_h2h_i_weight']
        h2h_f = net_params['lstm_h2h_f_weight']
        h2h_c = net_params['lstm_h2h_c_weight']
        h2h_o = net_params['lstm_h2h_o_weight']
        lstm = _utils.convert_lstm_weight_coreml_to_tf(i2h_i, i2h_c, i2h_f, i2h_o, h2h_i, h2h_c, h2h_f, h2h_o)
        return lstm


    def load_weights(self, net_params):
        """
        Function to load weights from the C++ implementation into TensorFlow

        Parameters
        ----------
        net_params: Dictionary
            Dict with weights from the C++ implementation and  its names

        """
        for key in net_params.keys():
            if key in self.weights.keys():
                if key.startswith('conv'):
                    net_params[key] = _utils.convert_conv1d_coreml_to_tf(net_params[key])
                    self.sess.run(_tf.assign(_tf.get_default_graph().get_tensor_by_name(key+":0"), net_params[key]))
                elif key.startswith('dense'):
                    net_params[key] = _utils.convert_dense_coreml_to_tf(net_params[key])
                    self.sess.run(_tf.assign(_tf.get_default_graph().get_tensor_by_name(key+":0"), net_params[key] ))
            elif key in self.biases.keys():
                self.sess.run(_tf.assign(_tf.get_default_graph().get_tensor_by_name(key+":0"), net_params[key]))

        h2h_i_bias = net_params['lstm_h2h_i_bias'] 
        h2h_c_bias = net_params['lstm_h2h_c_bias'] 
        h2h_f_bias = net_params['lstm_h2h_f_bias'] 
        h2h_o_bias = net_params['lstm_h2h_o_bias']
        lstm_bias = _utils.convert_lstm_bias_coreml_to_tf(h2h_i_bias, h2h_c_bias, h2h_f_bias, h2h_o_bias)
        self.sess.run(_tf.assign(_tf.get_default_graph().get_tensor_by_name('rnn/lstm_cell/bias:0'), lstm_bias))     

    def train(self, feed_dict):
        """
        Run session for training with new batch of data (inputs, labels and weights)

        Parameters
        ----------
        feed_dict: Dictionary
            Dictionary to store a batch of input data, corresponding labels and weights. This is currently
            passed from the ac_data_iterator.cpp file when a new batch of data is sent.

        Returns
        -------
        result: Dictionary
            Loss per batch and probabilities
        """
        for key in feed_dict.keys():
            feed_dict[key] = _utils.convert_shared_float_array_to_numpy(feed_dict[key])
            feed_dict[key] = _np.squeeze(feed_dict[key], axis=1)
            feed_dict[key] =  _np.reshape(feed_dict[key], (feed_dict[key].shape[0], feed_dict[key].shape[1], feed_dict[key].shape[2]))

        _, loss, probs = self.sess.run([self.train_op, self.loss_per_seq, self.probs], 
            feed_dict={self.data : feed_dict['input'], self.target : feed_dict['labels'], self.weight : feed_dict['weights'], self.is_training : True})
        
        prob = _np.array(probs)
        probabilities = _np.reshape(prob, (prob.shape[0], prob.shape[1]*prob.shape[2]))
        result = {'loss' : _np.array(loss), 'output': probabilities }
        return result

    def predict(self, feed_dict):

        """
        Run session for predicting with new batch of validation data (inputs, labels and weights) as well as test data (inputs)

        Parameters
        ----------
        feed_dict: Dictionary
            Dictionary to store a batch of input data, corresponding labels and weights. This is currently
            passed from the ac_data_iterator.cpp file when a new batch of data is sent.

        Returns
        -------
        result: Dictionary
            Loss per batch and probabilities (in case of validation data)
            Probabilities (in case only inputs are provided)
        """

        for key in feed_dict.keys():
            feed_dict[key] = _utils.convert_shared_float_array_to_numpy(feed_dict[key])
            feed_dict[key] = _np.squeeze(feed_dict[key], axis=1)
            feed_dict[key] =  _np.reshape(feed_dict[key], (feed_dict[key].shape[0], feed_dict[key].shape[1], feed_dict[key].shape[2]))

        if len(feed_dict.keys()) == 1:
            probs = self.sess.run(self.probs, 
                feed_dict={self.data : feed_dict['input'], self.is_training: False})
            prob = _np.array(probs)
            probabilities = _np.reshape(prob, (prob.shape[0], prob.shape[1]*prob.shape[2]))
            result  = { 'output' :  probabilities}    
        else:
            loss,  probs= self.sess.run([self.loss_per_seq,  self.probs], 
                feed_dict={self.data : feed_dict['input'], self.target : feed_dict['labels'], self.weight : feed_dict['weights'], self.is_training: False})
            prob = _np.array(probs)
            probabilities = _np.reshape(prob, (prob.shape[0], prob.shape[1]*prob.shape[2]))
            result = {'loss' : _np.array(loss),  'output': probabilities } 
        return result


    def export_weights(self):
        """
        Function to store TensorFlow weights back to into a dict in CoreML format to be used 
        by the C++ implementation

        Returns
        -------
        tf_export_params: Dictionary
            Dictionary of weights from TensorFlow stored as {weight_name: weight_value}
        """
        tf_export_params = {}
        tvars = _tf.trainable_variables()
        tvars_vals = self.sess.run(tvars)

        for var, val in zip(tvars, tvars_vals):
            if 'weight' in var.name:
                if var.name.startswith('conv'):
                    
                    tf_export_params[var.name.split(':')[0]] = _utils.convert_conv1d_tf_to_coreml(val)
                elif var.name.startswith('dense'):
                    tf_export_params[var.name.split(':')[0]] =  _utils.convert_dense_tf_to_coreml(val)
            elif var.name.startswith('rnn/lstm_cell/kernel'):
                i2h_i, i2h_c, i2h_f, i2h_o, h2h_i, h2h_c, h2h_f, h2h_o = _utils.convert_lstm_weight_tf_to_coreml(val, CONV_H)
                tf_export_params['lstm_i2h_i_weight'] = i2h_i
                tf_export_params['lstm_i2h_c_weight'] = i2h_c
                tf_export_params['lstm_i2h_f_weight'] = i2h_f
                tf_export_params['lstm_i2h_o_weight'] = i2h_o
                tf_export_params['lstm_h2h_i_weight'] = h2h_i
                tf_export_params['lstm_h2h_c_weight'] = h2h_c
                tf_export_params['lstm_h2h_f_weight'] = h2h_f
                tf_export_params['lstm_h2h_o_weight'] = h2h_o
            elif var.name.startswith('rnn/lstm_cell/bias'):
                h2h_i_bias, h2h_c_bias, h2h_f_bias, h2h_o_bias = _utils.convert_lstm_bias_tf_to_coreml(val)
                tf_export_params['lstm_h2h_i_bias'] = h2h_i_bias
                tf_export_params['lstm_h2h_c_bias'] = h2h_c_bias
                tf_export_params['lstm_h2h_f_bias'] = h2h_f_bias
                tf_export_params['lstm_h2h_o_bias'] = h2h_o_bias
            elif var.name.startswith('batch_normalization'):
                tf_export_params['bn_'+var.name.split('/')[-1][0:-2]] = _np.array(val)
            else:
                tf_export_params[var.name.split(':')[0]] = _np.array(val)
            
        tvars = _tf.all_variables()
        tvars_vals = self.sess.run(tvars)
        for var, val in zip(tvars, tvars_vals):
            if 'moving_mean' in var.name:
                tf_export_params['bn_running_mean'] = _np.array(val)
            if 'moving_variance' in var.name:
                tf_export_params['bn_running_var'] = _np.array(val)
        for layer_name in tf_export_params.keys():
            tf_export_params[layer_name] = _np.ascontiguousarray(tf_export_params[layer_name])
        return tf_export_params


    def set_learning_rate(self, lr):
        """
        Set the learning rate

        Parameters
        ----------
        lr: float32
            Learning rate

        """
        self.optimizer = _tf.train.AdamOptimizer(learning_rate=lr)
        